{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f11e23-fa12-487e-997a-1534b3ac710a",
   "metadata": {},
   "source": [
    "## Now, let's collect some fresh LLM annotations & human annotations (by annotating ourselves)!\n",
    "\n",
    "### We need to specify the follwoing parameters below, in code:\n",
    "\n",
    "### Changes to the parameters are the following:\n",
    "1. COLLECT_LLM = True\n",
    "2. COLLECT_HUMAN = True\n",
    "\n",
    "### We will also reduce the number of samples so that the annotation doesn't take long:\n",
    "1. N = 100\n",
    "2. burnin_steps = 5\n",
    "3. n_batches = 2\n",
    "4. n_human = 15\n",
    "5. sleep = 0.1\n",
    "\n",
    "### And, we will load the OpenAI key and the Mturk keys:\n",
    "- If you don't have those set up, just follow along!\n",
    "- Otherwise, fill in these three keys:\n",
    "    1. OPENAI_API_KEY = \"\"\n",
    "    2. AWS_ACCESS_KEY_ID = \"\"\n",
    "    3. AWS_SECRET_ACCESS_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c47f2f9-a220-47d7-9b39-fea05af70065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_credentials(file_path=\"credentials.txt\"):\n",
    "    credentials = {}\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if '=' in line:\n",
    "                key, value = line.strip().split('=', 1)\n",
    "                credentials[key.strip()] = value.strip()\n",
    "    return credentials\n",
    "\n",
    "# Load credentials\n",
    "creds = load_credentials()\n",
    "\n",
    "#put your key here\n",
    "AWS_ACCESS_KEY_ID = creds.get(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = creds.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "OPENAI_API_KEY = creds.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89875d1-3076-44bb-b7a2-200fa497520e",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782ce87f-8c5b-4e0b-a1bb-d592f3312eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, bernoulli\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from ppi_py.utils import bootstrap\n",
    "import re\n",
    "import openai\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import zipfile\n",
    "import io\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1b8551-bbe4-4c8a-b4ce-f0c808fd3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import llms, qualtrics, prolific, mturk, inference\n",
    "from utils.llms import annotate_texts_with_llm, collect_llm_confidence\n",
    "from utils.qualtrics import create_and_activate_surveys\n",
    "from utils.prolific import run_prolific_annotation_pipeline\n",
    "from utils.mturk import run_mturk_annotation_pipeline\n",
    "from utils.inference import train_sampling_rule, sampling_rule_predict, confidence_driven_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373c599-5e5e-4467-ba2f-ccb6ad81d38c",
   "metadata": {},
   "source": [
    "### Set parameters for Confidence-Driven Inference (CDI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e02c1a9-0d11-47fe-b158-4e2509a7479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if true, we collect LLM annotations and human annotations from scratch, if false, load pre-collected ones\n",
    "COLLECT_LLM = True\n",
    "COLLECT_HUMAN = True\n",
    "\n",
    "# if COLLECT_HUMAN = True, specify whether to use \"Prolific\" or \"MTURK\"\n",
    "HUMAN_SOURCE = \"MTURK\"\n",
    "\n",
    "alpha = 0.1  # desired error level for confidence interval\n",
    "burnin_steps = 5  # we collect the first warmup samples (initial set) to initialize sampling rule\n",
    "\n",
    "n_batches = 2\n",
    "\n",
    "tau = 0.1  # parameter for mixing with uniform sampling for increased stability\n",
    "n_human = 15 # budget on number of human annotations (including burnin_steps)\n",
    "\n",
    "N = 100 # corpus size, or the size of the random subset of the corpus that will be annotated with an LLM\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7403a9-1187-42e7-9e26-5e4feecdcbcd",
   "metadata": {},
   "source": [
    "### Set parameters for LLM annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36cd3394-f0dc-4da4-9826-ba01de5b1d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Is the following text polite? Output either A or B. Output a letter only.\n",
    "A) Polite\n",
    "B) Impolite\n",
    "\n",
    "Text: \"\"\"\n",
    "\n",
    "#category numerically coded as 1\n",
    "positive_class = 'Polite'\n",
    "\n",
    "mapping_categories = {\n",
    "\"A\": \"Polite\",\n",
    "\"B\": \"Impolite\"\n",
    "}\n",
    "\n",
    "model = \"gpt-4o-mini-2024-07-18\"\n",
    "sleep = 0.1\n",
    "temperature = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc1f229-b016-487e-a245-c36e9cccfbbc",
   "metadata": {},
   "source": [
    "### Set parameters for human annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a80cca2-0d6c-41fd-af39-45fa698d27cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task parameters\n",
    "task_title = \"Is the following text polite?\"\n",
    "annotation_instruction = \"Is the following text polite?\"\n",
    "task_description = \"Please annotate the politeness of the provided text. This is a real-time study where we're hoping to get immediate annotations.\"\n",
    "categories = [\n",
    "    \"Polite\",\n",
    "    \"Impolite\"\n",
    "]\n",
    "\n",
    "PROLIFIC_API_KEY = \"\"\n",
    "\n",
    "## Additional Prolific settings\n",
    "BASE_URL = \"https://api.prolific.com/api/v1\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Token {PROLIFIC_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "reward = 80\n",
    "estimated_time = 1\n",
    "maximum_allowed_time = 30 # How long a single annotator can take\n",
    "BATCH_TIMEOUT = 30 # How long we'll wait for the resonses (in minutes) before moving on (replaces not collected annotations with np.nan)\n",
    "\n",
    "## Additional MTURK settings\n",
    "task_reward = '0.8' \n",
    "minimum_approval_rate = 99 # >98% reccommended\n",
    "minimum_tasks_approved = 0 # >5000 reccommended\n",
    "annotation_instructions = {\"question\": task_description,\n",
    "    \"options\": set(categories)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef9902-4cd6-45cd-b9e2-24e22cfa1a20",
   "metadata": {},
   "source": [
    "### Step 1: Load the texts and collect LLM annotations\n",
    "\n",
    "#### Texts will be annotated for their politeness. We will showcase the estimation of the target statistic:\n",
    "Prevalence of the politeness $\\mathrm{mean}(Y)$, i.e., the fraction of texts in the corpus that are polite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1392bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_data_collection = time.time()\n",
    "\n",
    "# Load the text corpus; we need two columns: texts and the text-based feature we will use for inference\n",
    "# Text-based feature used for inference in this example is the presence of hedging, stored in \"Feature_3\" column\n",
    "text_based_feature = 'Feature_3'\n",
    "\n",
    "df = pd.read_csv('data/politeness_dataset.csv')[['Text',text_based_feature]]\n",
    "df = df.sample(n = N, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea039f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "data = pd.DataFrame()\n",
    "data['human'] = [np.nan]*(n)\n",
    "data['llm'] = [np.nan]*(n)\n",
    "data['llm_conf'] = [np.nan]*(n)\n",
    "data['X'] = df[text_based_feature].values\n",
    "data['text'] = df['Text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cfc30a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ece275c2b34821a3c2196462bc9ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting LLM annotations:   0%|          | 0/100 [00:00<?, ? annotation/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339da0866f6f4633b82ab1e228d5ec0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting LLM confidence:   0%|          | 0/100 [00:00<?, ? annotation/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if COLLECT_LLM:\n",
    "    #collect annotations\n",
    "    llm_annotated = annotate_texts_with_llm(texts = data['text'].values,\n",
    "                                       model = model,\n",
    "                                       prompt = prompt,\n",
    "                                       mapping_categories = mapping_categories,\n",
    "                                       sleep = sleep,\n",
    "                                       temperature = temperature,\n",
    "                                       OPENAI_API_KEY = OPENAI_API_KEY)\n",
    "    #collect verbalized confidence\n",
    "    llm_annotated_with_confidence = collect_llm_confidence(sample_texts = llm_annotated,\n",
    "                                       model = model,\n",
    "                                       sleep = sleep,\n",
    "                                       temperature = temperature,\n",
    "                                       OPENAI_API_KEY = OPENAI_API_KEY)\n",
    "\n",
    "    \n",
    "    data['llm'] = llm_annotated_with_confidence['LLM_annotation'].apply(lambda x: 1 if x.lower()==positive_class.lower() else 0).values\n",
    "    data['llm_conf'] = llm_annotated_with_confidence['confidence_in_prediction']\n",
    "else:\n",
    "    #load the extisting annotations we already collected\n",
    "    df['Prediction_gpt-4o'] = pd.read_csv('data/politeness_dataset.csv')['Prediction_gpt-4o'].sample(n = N, random_state = random_state).values\n",
    "    df['Confidence_gpt-4o'] = pd.read_csv('data/politeness_dataset.csv')['Confidence_gpt-4o'].sample(n = N, random_state = random_state).values\n",
    "    data['llm'] = df['Prediction_gpt-4o'].apply(lambda x: 1 if x.lower()==positive_class.lower() else 0).values\n",
    "    data['llm_conf'] = df['Confidence_gpt-4o'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d13cb2-b36e-452e-bad1-54e68038c54e",
   "metadata": {},
   "source": [
    "### Step 2: Collect human annotations for warmup samples (initial set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08063281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publishing anntotation tasks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b439c6af1b4a7d9c3458636654a56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can preview the hits here:\n",
      "https://workersandbox.mturk.com/mturk/preview?groupId=3SFRD5IA6L8IW7JM82P5P2CPSXPFB1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d817b2012d14af1a11d6ccf30e696cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completed HITs:   0%|          | 0/5 [00:00<?, ?HIT/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All HITs are completed.\n"
     ]
    }
   ],
   "source": [
    "if COLLECT_HUMAN:\n",
    "    texts_to_annotate = list(data.loc[:burnin_steps-1,'text'].values)\n",
    "\n",
    "    if HUMAN_SOURCE == \"Prolific\":\n",
    "    \n",
    "        # create Qualtrics annotation interface and get annotation task URLs\n",
    "        survey_links = create_and_activate_surveys(\n",
    "            texts_to_annotate=texts_to_annotate,\n",
    "            categories=categories,\n",
    "            annotation_instruction=annotation_instruction,\n",
    "            QUALTRICS_API_URL=QUALTRICS_API_URL,\n",
    "            QUALTRICS_API_KEY=QUALTRICS_API_KEY)\n",
    "        \n",
    "        # run the Prolific annotation pipeline\n",
    "        annotations = run_prolific_annotation_pipeline(\n",
    "            survey_links=list(survey_links.values()),\n",
    "            name_prefix=task_title,\n",
    "            description=task_description,\n",
    "            reward = reward,  \n",
    "            estimated_time=estimated_time,\n",
    "            max_time = maximum_allowed_time,\n",
    "            HEADERS = HEADERS,\n",
    "            BASE_URL = BASE_URL,\n",
    "            QUALTRICS_API_URL = QUALTRICS_API_URL,\n",
    "            QUALTRICS_API_KEY = QUALTRICS_API_KEY,\n",
    "            BATCH_TIMEOUT = BATCH_TIMEOUT\n",
    "        )\n",
    "\n",
    "    if HUMAN_SOURCE == \"MTURK\":\n",
    "        annotations = run_mturk_annotation_pipeline(pd.DataFrame(texts_to_annotate, columns=['Text']),\n",
    "                                            annotation_instructions = annotation_instructions,\n",
    "                                            task_title = task_title,\n",
    "                                            task_description = task_description,\n",
    "                                            task_reward = task_reward,\n",
    "                                            minimum_approval_rate = minimum_approval_rate,\n",
    "                                            minimum_tasks_approved = minimum_tasks_approved,\n",
    "                                            aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
    "                                            aws_secret_access_key = AWS_SECRET_ACCESS_KEY)\n",
    "        \n",
    "    data.loc[:burnin_steps-1,'human'] = pd.Series(annotations).apply(lambda x: 1 if x.lower()==positive_class.lower() else 0).values\n",
    "else:\n",
    "    #load the extisting annotations we already collected\n",
    "    df['Prediction_human'] = pd.read_csv('data/politeness_dataset.csv').sample(n = N, random_state = random_state)['Prediction_human'].values\n",
    "    df = df['Prediction_human'].reset_index()\n",
    "\n",
    "    #initialize the first warmup samples (initial set)\n",
    "    data.loc[:burnin_steps-1,'human'] = df['Prediction_human'].values[:burnin_steps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b242ad9-9bc5-4c97-a83c-b80434e6cef1",
   "metadata": {},
   "source": [
    "### Step 2: Initialize the sampling rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ec69d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = data['llm_conf'].to_numpy().reshape((n,1))\n",
    "confidence_burnin = confidence[:burnin_steps]\n",
    "H = data['human'].to_numpy()\n",
    "H_burnin = H[:burnin_steps]\n",
    "Hhat = data['llm'].to_numpy()\n",
    "Hhat_burnin = Hhat[:burnin_steps]\n",
    "SP = np.zeros(n)\n",
    "SD = np.zeros(n)\n",
    "SP[:burnin_steps] = np.ones(burnin_steps)\n",
    "SD[:burnin_steps] = np.ones(burnin_steps)\n",
    "sampling_rule = train_sampling_rule(confidence_burnin, (H_burnin - Hhat_burnin)**2) # trains XGBoost model\n",
    "sampling_probs_unnormed = np.clip(np.sqrt(sampling_rule_predict(sampling_rule, confidence)), 1e-4, 1)\n",
    "avg_sampling_probs = np.mean(sampling_probs_unnormed)\n",
    "frac_human_adjusted = (n_human - burnin_steps)/(n - burnin_steps) # remove warmup samples (initial set) from available budget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d94f52-61c7-470a-ba92-04112cf58cfb",
   "metadata": {},
   "source": [
    "### Step 3: In batches, strategically sample texts for human annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0ffa9e9-d1af-41d7-ac01-e682110d81b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting batch 1/2...\n",
      "Publishing anntotation tasks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8aea3b94ba44803baa1b86170299a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can preview the hits here:\n",
      "https://workersandbox.mturk.com/mturk/preview?groupId=3SFRD5IA6L8IW7JM82P5P2CPSXPFB1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231ef1b2834843d4bd09491a6bf7a0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completed HITs:   0%|          | 0/4 [00:00<?, ?HIT/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All HITs are completed.\n",
      "\n",
      "Collecting batch 2/2...\n",
      "Publishing anntotation tasks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a503dce4707f44b9892efb43b08d7f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can preview the hits here:\n",
      "https://workersandbox.mturk.com/mturk/preview?groupId=3SFRD5IA6L8IW7JM82P5P2CPSXPFB1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe65134c00f044fcb223daa31b925717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completed HITs:   0%|          | 0/5 [00:00<?, ?HIT/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All HITs are completed.\n"
     ]
    }
   ],
   "source": [
    "batch_size = (n - burnin_steps)//n_batches\n",
    "for b in range(n_batches):\n",
    "    if b < (n_batches - 1):\n",
    "        batch_inds = np.array(range(burnin_steps + b*batch_size, burnin_steps + (b+1)*batch_size))\n",
    "    else:\n",
    "        batch_inds = np.array(range(burnin_steps + b*batch_size, n))\n",
    "        \n",
    "    sampling_probs = sampling_probs_unnormed[batch_inds]/avg_sampling_probs*frac_human_adjusted\n",
    "    sampling_probs = np.clip((1-tau)*sampling_probs + tau*frac_human_adjusted, 0, 1)\n",
    "\n",
    "    if np.isnan(sampling_probs).all():\n",
    "        print(f\"Training the model failed at batch {b+1}/{n_batches}... Quitting.\")\n",
    "        break\n",
    "        \n",
    "    labeling_decisions = bernoulli.rvs(sampling_probs)\n",
    "    indices_to_label = batch_inds[np.where(labeling_decisions)]\n",
    "\n",
    "    print()\n",
    "    print(f\"Collecting batch {b+1}/{n_batches}...\")\n",
    "    if COLLECT_HUMAN:\n",
    "        texts_to_annotate = list(data.loc[indices_to_label,'text'].values)\n",
    "\n",
    "        if HUMAN_SOURCE == \"Prolific\":\n",
    "            # create Qualtrics annotation interface and get annotation task URLs\n",
    "            survey_links = create_and_activate_surveys(\n",
    "                texts_to_annotate=texts_to_annotate,\n",
    "                categories=categories,\n",
    "                annotation_instruction=annotation_instruction,\n",
    "                QUALTRICS_API_URL=QUALTRICS_API_URL,\n",
    "                QUALTRICS_API_KEY=QUALTRICS_API_KEY)\n",
    "            \n",
    "            # run the Prolific annotation pipeline\n",
    "            annotations = run_prolific_annotation_pipeline(\n",
    "                survey_links=list(survey_links.values()),\n",
    "                name_prefix=task_title,\n",
    "                description=task_description,\n",
    "                reward = reward,  \n",
    "                estimated_time=estimated_time,\n",
    "                max_time = maximum_allowed_time,\n",
    "                HEADERS = HEADERS,\n",
    "                BASE_URL = BASE_URL,\n",
    "                QUALTRICS_API_URL = QUALTRICS_API_URL,\n",
    "                QUALTRICS_API_KEY = QUALTRICS_API_KEY,\n",
    "                BATCH_TIMEOUT = BATCH_TIMEOUT\n",
    "            )\n",
    "\n",
    "        if HUMAN_SOURCE == \"MTURK\":\n",
    "            annotations = run_mturk_annotation_pipeline(pd.DataFrame(texts_to_annotate, columns=['Text']),\n",
    "                                            annotation_instructions = annotation_instructions,\n",
    "                                            task_title = task_title,\n",
    "                                            task_description = task_description,\n",
    "                                            task_reward = task_reward,\n",
    "                                            minimum_approval_rate = minimum_approval_rate,\n",
    "                                            minimum_tasks_approved = minimum_tasks_approved,\n",
    "                                            aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
    "                                            aws_secret_access_key = AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "        H[indices_to_label] = pd.Series(annotations).apply(lambda x: 1 if x.lower()==positive_class.lower() else 0).values\n",
    "    else: \n",
    "        H[indices_to_label] = df['Prediction_human'].iloc[list(indices_to_label)]\n",
    "        print(f\"Collecting {len(df['Prediction_human'].iloc[list(indices_to_label)])} human annotations.\") \n",
    "\n",
    "    collected_inds = np.where(labeling_decisions)\n",
    "\n",
    "    SP[batch_inds] = sampling_probs\n",
    "    SD[batch_inds] = labeling_decisions\n",
    "    \n",
    "    if b < (n_batches - 1):\n",
    "        sampling_rule = train_sampling_rule(confidence[collected_inds], (H[collected_inds] - Hhat[collected_inds])**2)\n",
    "        sampling_probs_unnormed = np.clip(np.sqrt(sampling_rule_predict(sampling_rule, confidence)), 1e-4, 1)\n",
    "        avg_sampling_probs = np.mean(sampling_probs_unnormed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eda782be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 human datapoints collected in total.\n"
     ]
    }
   ],
   "source": [
    "data.loc[list(collected_inds[0]),'human'] = H[list(collected_inds)][0]\n",
    "data['sampling_probs'] = SP\n",
    "data['sampling_decisions'] = SD\n",
    "print(f\"{len(data.dropna(subset = ['human']))} human datapoints collected in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1fca34d-d217-4275-8ef3-60c033f6a851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human</th>\n",
       "      <th>llm</th>\n",
       "      <th>llm_conf</th>\n",
       "      <th>X</th>\n",
       "      <th>text</th>\n",
       "      <th>sampling_probs</th>\n",
       "      <th>sampling_decisions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>thank you for that. Do you think it is possibl...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>Notability is an issue of common sense: you're...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>Too vague - explain what you expect the conten...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>I am the third party; &lt;url&gt; was the first to d...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>Interesting problem.  What have you tried so f...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>Thanks for approving my request. Should I remo...</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "      <td>What do you mean by MobileMail? The Mail app o...</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "      <td>Please see our ongoing discussion &lt;url&gt;. Is th...</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "      <td>You really want off here, dont you? I've alrea...</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>So who exactly is going to *teach* this topic ...</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>The PressTV references in Wikipedia's \"Turkey-...</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>It's going to depend (to a large degree I woul...</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "      <td>@DSchultz Tried adding the 'www' (no luck). Ho...</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "      <td>Alright Mike - its been already posted on ligh...</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    human  llm llm_conf  X                                               text  \\\n",
       "0     1.0    1      0.8  1  thank you for that. Do you think it is possibl...   \n",
       "1     0.0    0      0.2  0  Notability is an issue of common sense: you're...   \n",
       "2     0.0    0      0.3  1  Too vague - explain what you expect the conten...   \n",
       "3     0.0    0      0.2  0  I am the third party; <url> was the first to d...   \n",
       "4     1.0    1      0.9  0  Interesting problem.  What have you tried so f...   \n",
       "10    1.0    1      0.9  1  Thanks for approving my request. Should I remo...   \n",
       "39    1.0    1      0.7  0  What do you mean by MobileMail? The Mail app o...   \n",
       "42    0.0    0      0.7  0  Please see our ongoing discussion <url>. Is th...   \n",
       "51    0.0    0      0.7  0  You really want off here, dont you? I've alrea...   \n",
       "58    0.0    0      0.3  1  So who exactly is going to *teach* this topic ...   \n",
       "65    1.0    1      0.8  0  The PressTV references in Wikipedia's \"Turkey-...   \n",
       "75    1.0    1      0.7  1  It's going to depend (to a large degree I woul...   \n",
       "80    1.0    1      0.7  0  @DSchultz Tried adding the 'www' (no luck). Ho...   \n",
       "99    1.0    1      0.7  0  Alright Mike - its been already posted on ligh...   \n",
       "\n",
       "    sampling_probs  sampling_decisions  \n",
       "0         1.000000                 1.0  \n",
       "1         1.000000                 1.0  \n",
       "2         1.000000                 1.0  \n",
       "3         1.000000                 1.0  \n",
       "4         1.000000                 1.0  \n",
       "10        0.105263                 1.0  \n",
       "39        0.105263                 1.0  \n",
       "42        0.105263                 1.0  \n",
       "51        0.105263                 1.0  \n",
       "58        0.105263                 1.0  \n",
       "65        0.105263                 1.0  \n",
       "75        0.105263                 1.0  \n",
       "80        0.105263                 1.0  \n",
       "99        0.105263                 1.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[~data['human'].isna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cac03860-80e5-41ea-9820-4a5a508d6683",
   "metadata": {},
   "source": [
    "### Step 4: Compute the estimate and confidence interval\n",
    "#### We showcase estimation of $\\mathrm{mean}(Y)$: prevalence of the politeness, i.e., the fraction of texts in the corpus that are polite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05da928d-6afa-4ab8-a875-306c75ed068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the estimator function\n",
    "#mask for valid labels and specify how to use weights\n",
    "\n",
    "def mean_estimator(y, weights):\n",
    "    y, weights = y[~np.isnan(y)], weights[~np.isnan(y)]\n",
    "    return np.sum(y * weights) / np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4049879-79e5-44b2-a85e-38c7c1be3569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDI estimate of the target statistic (mean(Y): prevalence of politeness):\n",
      "point estimate: 0.5793\n",
      "confidence intervals: 0.4963 0.6624\n"
     ]
    }
   ],
   "source": [
    "estimate, (lower_bound, upper_bound) = confidence_driven_inference(\n",
    "    estimator = mean_estimator,\n",
    "    Y = data['human'].values,\n",
    "    Yhat = data['llm'].values,\n",
    "    sampling_probs =  data['sampling_probs'].values,\n",
    "    sampling_decisions = data['sampling_decisions'].values,\n",
    "    alpha = alpha)\n",
    "\n",
    "print(\"CDI estimate of the target statistic (mean(Y): prevalence of politeness):\")\n",
    "print('point estimate:',estimate.round(4))\n",
    "print('confidence intervals:', lower_bound.round(4), upper_bound.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c58f9567-0d33-4cfe-a4ec-3a324185d273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth mean(Y) estimate (if we had access to human annotations on the full text corpus):\n",
      "0.58\n"
     ]
    }
   ],
   "source": [
    "print(\"Ground truth mean(Y) estimate (if we had access to human annotations on the full text corpus):\")\n",
    "print(np.mean(pd.read_csv('data/politeness_dataset.csv').sample(n = N, random_state = random_state)['Prediction_human'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c25244d6-8afe-4e64-a165-c5a30b170b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human</th>\n",
       "      <th>llm</th>\n",
       "      <th>llm_conf</th>\n",
       "      <th>X</th>\n",
       "      <th>text</th>\n",
       "      <th>sampling_probs</th>\n",
       "      <th>sampling_decisions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>thank you for that. Do you think it is possibl...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>Notability is an issue of common sense: you're...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>Too vague - explain what you expect the conten...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>I am the third party; &lt;url&gt; was the first to d...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>Interesting problem.  What have you tried so f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   human  llm llm_conf  X                                               text  \\\n",
       "0    1.0    1      0.8  1  thank you for that. Do you think it is possibl...   \n",
       "1    0.0    0      0.2  0  Notability is an issue of common sense: you're...   \n",
       "2    0.0    0      0.3  1  Too vague - explain what you expect the conten...   \n",
       "3    0.0    0      0.2  0  I am the third party; <url> was the first to d...   \n",
       "4    1.0    1      0.9  0  Interesting problem.  What have you tried so f...   \n",
       "\n",
       "   sampling_probs  sampling_decisions  \n",
       "0             1.0                 1.0  \n",
       "1             1.0                 1.0  \n",
       "2             1.0                 1.0  \n",
       "3             1.0                 1.0  \n",
       "4             1.0                 1.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58e4de4c-ed83-40f4-8a7d-f8f8f2300f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data collection time: 6.09 minutes.\n"
     ]
    }
   ],
   "source": [
    "elapsed_minutes = (time.time() - start_time_data_collection) / 60\n",
    "print(f\"Total data collection time: {elapsed_minutes:.2f} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
