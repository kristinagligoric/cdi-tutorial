{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89875d1-3076-44bb-b7a2-200fa497520e",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782ce87f-8c5b-4e0b-a1bb-d592f3312eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, bernoulli\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from ppi_py.utils import bootstrap\n",
    "import re\n",
    "import openai\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import zipfile\n",
    "import io\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1b8551-bbe4-4c8a-b4ce-f0c808fd3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import llms, qualtrics, prolific, mturk, inference\n",
    "from utils.llms import annotate_texts_with_llm, collect_llm_confidence\n",
    "from utils.qualtrics import create_and_activate_surveys\n",
    "from utils.prolific import run_prolific_annotation_pipeline\n",
    "from utils.mturk import run_mturk_annotation_pipeline\n",
    "from utils.inference import train_sampling_rule, sampling_rule_predict, confidence_driven_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373c599-5e5e-4467-ba2f-ccb6ad81d38c",
   "metadata": {},
   "source": [
    "### Set parameters for Confidence-Driven Inference (CDI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e02c1a9-0d11-47fe-b158-4e2509a7479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if true, we collect LLM annotations and human annotations from scratch, if false, load pre-collected ones\n",
    "COLLECT_LLM = False\n",
    "COLLECT_HUMAN = False\n",
    "\n",
    "# if COLLECT_HUMAN = True, specify whether to use \"Prolific\" or \"MTURK\"\n",
    "HUMAN_SOURCE = \"MTURK\"\n",
    "\n",
    "alpha = 0.1  # desired error level for confidence interval\n",
    "burnin_steps = 50  # we collect the first burnin_steps points to initialize sampling rule\n",
    "\n",
    "n_batches = 4\n",
    "\n",
    "tau = 0.1  # parameter for mixing with uniform sampling for increased stability\n",
    "n_human = 200 # budget on number of human annotations (including burnin_steps)\n",
    "\n",
    "N = 1000 # corpus size, or the size of the random subset of the corpus that will be annotated with an LLM\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929fb794-4626-4ee2-b50d-26e1be22cc20",
   "metadata": {},
   "source": [
    "### Set API keys (needed only if COLLECT_LLM or COLLECT_HUMAN are True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c47f2f9-a220-47d7-9b39-fea05af70065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Open API key\n",
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "# Set your Qualtrics API key and url here\n",
    "QUALTRICS_API_KEY = \"\"\n",
    "\n",
    "# Set your Qualtrics url here (e.g., https://stanforduniversity.qualtrics.com/API/v3)\n",
    "QUALTRICS_API_URL = \"\"\n",
    "\n",
    "# Set your Prolific API key here\n",
    "PROLIFIC_API_KEY = \"\"\n",
    "\n",
    "# Set your MTURK API key here\n",
    "AWS_ACCESS_KEY_ID = \"\"\n",
    "\n",
    "# Set your MTURK API secret access key here\n",
    "AWS_SECRET_ACCESS_KEY = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7403a9-1187-42e7-9e26-5e4feecdcbcd",
   "metadata": {},
   "source": [
    "### Set parameters for LLM annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36cd3394-f0dc-4da4-9826-ba01de5b1d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Is the following text polite? Output either A or B. Output a letter only.\n",
    "A) Polite\n",
    "B) Impolite\n",
    "\n",
    "Text: \"\"\"\n",
    "\n",
    "#category numerically coded as 1\n",
    "positive_class = 'Polite'\n",
    "\n",
    "mapping_categories = {\n",
    "\"A\": \"Polite\",\n",
    "\"B\": \"Impolite\"\n",
    "}\n",
    "\n",
    "model = \"gpt-4o-mini-2024-07-18\"\n",
    "sleep = 0.5\n",
    "temperature = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc1f229-b016-487e-a245-c36e9cccfbbc",
   "metadata": {},
   "source": [
    "### Set parameters for human annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a80cca2-0d6c-41fd-af39-45fa698d27cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task parameters\n",
    "task_title = \"Is the following text polite?\"\n",
    "annotation_instruction = \"Is the following text polite?\"\n",
    "task_description = \"Please annotate the politeness of the provided text. This is a real-time study where we're hoping to get immediate annotations.\"\n",
    "categories = [\n",
    "    \"Polite\",\n",
    "    \"Impolite\"\n",
    "]\n",
    "\n",
    "## Additional Prolific settings\n",
    "BASE_URL = \"https://api.prolific.com/api/v1\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Token {PROLIFIC_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "reward = 80\n",
    "estimated_time = 1\n",
    "maximum_allowed_time = 30 # How long a single annotator can take\n",
    "BATCH_TIMEOUT = 30 # How long we'll wait for the resonses (in minutes) before moving on (replaces not collected annotations with np.nan)\n",
    "\n",
    "## Additional MTURK settings\n",
    "task_reward = '0.8' \n",
    "minimum_approval_rate = 99 # >98% reccommended\n",
    "minimum_tasks_approved = 0 # >5000 reccommended\n",
    "annotation_instructions = {\"question\": task_description,\n",
    "    \"options\": set(categories)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef9902-4cd6-45cd-b9e2-24e22cfa1a20",
   "metadata": {},
   "source": [
    "### Step 1: Load the texts and collect LLM annotations\n",
    "\n",
    "#### Texts will be annotated for their politeness. We will showcase the estimation of two target statistics:\n",
    "1. Prevalence of the politeness $mean(Y)$, i.e., the fraction of texts in the corpus that are polite\n",
    "2. The impact of text-based feature, i.e., presence of hedging ($X$), on the perceived politeness ($Y$), estimated with a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1392bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_data_collection = time.time()\n",
    "\n",
    "# Load the text corpus; we need two columns: texts and the text-based feature we will use for inference\n",
    "# Text-based feature used for inference in this example is the presence of hedging, stored in \"Feature_3\" column\n",
    "text_based_feature = 'Feature_3'\n",
    "\n",
    "df = pd.read_csv('data/politeness_dataset.csv')[['Text',text_based_feature]]\n",
    "df = df.sample(n = N, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea039f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "data = pd.DataFrame()\n",
    "data['human'] = [np.nan]*(n)\n",
    "data['llm'] = [np.nan]*(n)\n",
    "data['llm_conf'] = [np.nan]*(n)\n",
    "data['X'] = df[text_based_feature].values\n",
    "data['text'] = df['Text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cfc30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLLECT_LLM:\n",
    "    #collect annotations\n",
    "    sample_texts = annotate_texts_with_llm(texts = data['text'].values,\n",
    "                                       model = model,\n",
    "                                       prompt = prompt,\n",
    "                                       mapping_categories = mapping_categories,\n",
    "                                       sleep = sleep,\n",
    "                                       temperature = temperature,\n",
    "                                       OPENAI_API_KEY = OPENAI_API_KEY)\n",
    "    #collect verbalized confidence\n",
    "    sample_texts = collect_llm_confidence(sample_texts = sample_texts,\n",
    "                                       model = model,\n",
    "                                       sleep = sleep,\n",
    "                                       temperature = temperature,\n",
    "                                       OPENAI_API_KEY = OPENAI_API_KEY)\n",
    "\n",
    "    \n",
    "    data['llm'] = sample_texts['LLM_annotation'].apply(lambda x: 1 if x.lower()==positive_class.lower() else 0).values\n",
    "    data['llm_conf'] = sample_texts['confidence_in_prediction']\n",
    "else:\n",
    "    #load the extisting annotations we already collected\n",
    "    df['Prediction_gpt-4o'] = pd.read_csv('data/politeness_dataset.csv')['Prediction_gpt-4o'].sample(n = N, random_state = random_state).values\n",
    "    df['Confidence_gpt-4o'] = pd.read_csv('data/politeness_dataset.csv')['Confidence_gpt-4o'].sample(n = N, random_state = random_state).values\n",
    "    data['llm'] = df['Prediction_gpt-4o'].apply(lambda x: 1 if x.lower()==positive_class.lower() else 0).values\n",
    "    data['llm_conf'] = df['Confidence_gpt-4o'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d13cb2-b36e-452e-bad1-54e68038c54e",
   "metadata": {},
   "source": [
    "### Step 2: Collect human annotations for burnin steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08063281",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLLECT_HUMAN:\n",
    "    texts_to_annotate = list(data.loc[:burnin_steps-1,'text'].values)\n",
    "\n",
    "    if HUMAN_SOURCE == \"Prolific\":\n",
    "    \n",
    "        # create Qualtrics annotation interface and get annotation task URLs\n",
    "        survey_links = create_and_activate_surveys(\n",
    "            texts_to_annotate=texts_to_annotate,\n",
    "            categories=categories,\n",
    "            annotation_instruction=annotation_instruction,\n",
    "            QUALTRICS_API_URL=QUALTRICS_API_URL,\n",
    "            QUALTRICS_API_KEY=QUALTRICS_API_KEY)\n",
    "        \n",
    "        # run the Prolific annotation pipeline\n",
    "        annotations = run_prolific_annotation_pipeline(\n",
    "            survey_links=list(survey_links.values()),\n",
    "            name_prefix=task_title,\n",
    "            description=task_description,\n",
    "            reward = reward,  \n",
    "            estimated_time=estimated_time,\n",
    "            max_time = maximum_allowed_time,\n",
    "            HEADERS = HEADERS,\n",
    "            BASE_URL = BASE_URL,\n",
    "            QUALTRICS_API_URL = QUALTRICS_API_URL,\n",
    "            QUALTRICS_API_KEY = QUALTRICS_API_KEY,\n",
    "            BATCH_TIMEOUT = BATCH_TIMEOUT\n",
    "        )\n",
    "\n",
    "    if HUMAN_SOURCE == \"MTURK\":\n",
    "        annotations = run_mturk_annotation_pipeline(pd.DataFrame(texts_to_annotate, columns=['Text']),\n",
    "                                            annotation_instructions = annotation_instructions,\n",
    "                                            task_title = task_title,\n",
    "                                            task_description = task_description,\n",
    "                                            task_reward = task_reward,\n",
    "                                            minimum_approval_rate = minimum_approval_rate,\n",
    "                                            minimum_tasks_approved = minimum_tasks_approved,\n",
    "                                            aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
    "                                            aws_secret_access_key = AWS_SECRET_ACCESS_KEY)\n",
    "        \n",
    "    data.loc[:burnin_steps-1,'human'] = pd.Series(annotations).apply(lambda x: 1 if x.lower()==positive_class.lower() else 0).values\n",
    "else:\n",
    "    #load the extisting annotations we already collected\n",
    "    df['Prediction_human'] = pd.read_csv('data/politeness_dataset.csv').sample(n = N, random_state = random_state)['Prediction_human'].values\n",
    "    df = df['Prediction_human'].reset_index()\n",
    "\n",
    "    #initialize the first burnin_steps annotations\n",
    "    data.loc[:burnin_steps-1,'human'] = df['Prediction_human'].values[:burnin_steps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b242ad9-9bc5-4c97-a83c-b80434e6cef1",
   "metadata": {},
   "source": [
    "### Step 2: Initialize the sampling rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ec69d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = data['llm_conf'].to_numpy().reshape((n,1))\n",
    "confidence_burnin = confidence[:burnin_steps]\n",
    "H = data['human'].to_numpy()\n",
    "H_burnin = H[:burnin_steps]\n",
    "Hhat = data['llm'].to_numpy()\n",
    "Hhat_burnin = Hhat[:burnin_steps]\n",
    "SP = np.zeros(n)\n",
    "SD = np.zeros(n)\n",
    "SP[:burnin_steps] = np.ones(burnin_steps)\n",
    "SD[:burnin_steps] = np.ones(burnin_steps)\n",
    "sampling_rule = train_sampling_rule(confidence_burnin, (H_burnin - Hhat_burnin)**2) # trains XGBoost model\n",
    "sampling_probs_unnormed = np.clip(np.sqrt(sampling_rule_predict(sampling_rule, confidence)), 1e-4, 1)\n",
    "avg_sampling_probs = np.mean(sampling_probs_unnormed)\n",
    "frac_human_adjusted = (n_human - burnin_steps)/(n - burnin_steps) # remove burnin_steps samples from available budget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d94f52-61c7-470a-ba92-04112cf58cfb",
   "metadata": {},
   "source": [
    "### Step 3: In batches, strategically sample texts for human annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0ffa9e9-d1af-41d7-ac01-e682110d81b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting batch 1/4...\n",
      "Collecting 33 human annotations.\n",
      "\n",
      "Collecting batch 2/4...\n",
      "Collecting 43 human annotations.\n",
      "\n",
      "Collecting batch 3/4...\n",
      "Collecting 38 human annotations.\n",
      "\n",
      "Collecting batch 4/4...\n",
      "Collecting 41 human annotations.\n"
     ]
    }
   ],
   "source": [
    "batch_size = (n - burnin_steps)//n_batches\n",
    "for b in range(n_batches):\n",
    "    if b < (n_batches - 1):\n",
    "        batch_inds = np.array(range(burnin_steps + b*batch_size, burnin_steps + (b+1)*batch_size))\n",
    "    else:\n",
    "        batch_inds = np.array(range(burnin_steps + b*batch_size, n))\n",
    "        \n",
    "    sampling_probs = sampling_probs_unnormed[batch_inds]/avg_sampling_probs*frac_human_adjusted\n",
    "    sampling_probs = np.clip((1-tau)*sampling_probs + tau*frac_human_adjusted, 0, 1)\n",
    "\n",
    "    if np.isnan(sampling_probs).all():\n",
    "        print(f\"Training the model failed at batch {b+1}/{n_batches}... Quitting.\")\n",
    "        break\n",
    "        \n",
    "    labeling_decisions = bernoulli.rvs(sampling_probs)\n",
    "    indices_to_label = batch_inds[np.where(labeling_decisions)]\n",
    "\n",
    "    print()\n",
    "    print(f\"Collecting batch {b+1}/{n_batches}...\")\n",
    "    if COLLECT_HUMAN:\n",
    "        texts_to_annotate = list(data.loc[indices_to_label,'text'].values)\n",
    "\n",
    "        if HUMAN_SOURCE == \"Prolific\":\n",
    "            # create Qualtrics annotation interface and get annotation task URLs\n",
    "            survey_links = create_and_activate_surveys(\n",
    "                texts_to_annotate=texts_to_annotate,\n",
    "                categories=categories,\n",
    "                annotation_instruction=annotation_instruction,\n",
    "                QUALTRICS_API_URL=QUALTRICS_API_URL,\n",
    "                QUALTRICS_API_KEY=QUALTRICS_API_KEY)\n",
    "            \n",
    "            # run the Prolific annotation pipeline\n",
    "            annotations = run_prolific_annotation_pipeline(\n",
    "                survey_links=list(survey_links.values()),\n",
    "                name_prefix=task_title,\n",
    "                description=task_description,\n",
    "                reward = reward,  \n",
    "                estimated_time=estimated_time,\n",
    "                max_time = maximum_allowed_time,\n",
    "                HEADERS = HEADERS,\n",
    "                BASE_URL = BASE_URL,\n",
    "                QUALTRICS_API_URL = QUALTRICS_API_URL,\n",
    "                QUALTRICS_API_KEY = QUALTRICS_API_KEY,\n",
    "                BATCH_TIMEOUT = BATCH_TIMEOUT\n",
    "            )\n",
    "\n",
    "        if HUMAN_SOURCE == \"MTURK\":\n",
    "            annotations = run_mturk_annotation_pipeline(pd.DataFrame(texts_to_annotate, columns=['Text']),\n",
    "                                            annotation_instructions = annotation_instructions,\n",
    "                                            task_title = task_title,\n",
    "                                            task_description = task_description,\n",
    "                                            task_reward = task_reward,\n",
    "                                            minimum_approval_rate = minimum_approval_rate,\n",
    "                                            minimum_tasks_approved = minimum_tasks_approved,\n",
    "                                            aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
    "                                            aws_secret_access_key = AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "        H[indices_to_label] = pd.Series(annotations).apply(lambda x: 1 if x.lower()==positive_class.lower() else 0).values\n",
    "    else: \n",
    "        H[indices_to_label] = df['Prediction_human'].iloc[list(indices_to_label)]\n",
    "        print(f\"Collecting {len(df['Prediction_human'].iloc[list(indices_to_label)])} human annotations.\") \n",
    "\n",
    "    collected_inds = np.where(labeling_decisions)\n",
    "\n",
    "    SP[batch_inds] = sampling_probs\n",
    "    SD[batch_inds] = labeling_decisions\n",
    "    \n",
    "    if b < (n_batches - 1):\n",
    "        sampling_rule = train_sampling_rule(confidence[collected_inds], (H[collected_inds] - Hhat[collected_inds])**2)\n",
    "        sampling_probs_unnormed = np.clip(np.sqrt(sampling_rule_predict(sampling_rule, confidence)), 1e-4, 1)\n",
    "        avg_sampling_probs = np.mean(sampling_probs_unnormed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eda782be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205 human datapoints collected in total.\n"
     ]
    }
   ],
   "source": [
    "data.loc[list(collected_inds[0]),'human'] = H[list(collected_inds)][0]\n",
    "data['sampling_probs'] = SP\n",
    "data['sampling_decisions'] = SD\n",
    "print(f\"{len(data.dropna(subset = ['human']))} human datapoints collected in total.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cac03860-80e5-41ea-9820-4a5a508d6683",
   "metadata": {},
   "source": [
    "### Step 4: Compute the estimate and confidence interval\n",
    "#### We showcase estimation of two target statistics:\n",
    "1. $mean(Y)$: prevalence of the politeness, i.e., the fraction of texts in the corpus that are polite\n",
    "2. $\\beta_{hedge}$: the impact of linguistic features of hedging ($X$) on the perceived politeness ($Y$), estimated with a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05da928d-6afa-4ab8-a875-306c75ed068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the estimator function\n",
    "#mask for valid labels and specify how to use weights\n",
    "\n",
    "def mean_estimator(y, weights):\n",
    "    y, weights = y[~np.isnan(y)], weights[~np.isnan(y)]\n",
    "    return np.sum(y * weights) / np.sum(weights)\n",
    "\n",
    "def log_reg_estimator(X, y, weights):\n",
    "    X, y, weights = X[~np.isnan(y)], y[~np.isnan(y)], weights[~np.isnan(y)]\n",
    "    return LogisticRegression(solver=\"liblinear\").fit(X, y, sample_weight=weights).coef_[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d935732a-18af-42ff-848c-3929595fe0ef",
   "metadata": {},
   "source": [
    "#### $mean(Y)$ estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4049879-79e5-44b2-a85e-38c7c1be3569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDI estimate of the target statistic (mean(Y): prevalence of politeness):\n",
      "0.3993 0.5256\n"
     ]
    }
   ],
   "source": [
    "lower_bound, upper_bound = confidence_driven_inference(\n",
    "    estimator = mean_estimator,\n",
    "    Y = data['human'].values,\n",
    "    Yhat = data['llm'].values,\n",
    "    sampling_probs =  data['sampling_probs'].values,\n",
    "    sampling_decisions = data['sampling_decisions'].values)\n",
    "\n",
    "print(\"CDI estimate of the target statistic (mean(Y): prevalence of politeness):\")\n",
    "print(lower_bound.round(4), upper_bound.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c58f9567-0d33-4cfe-a4ec-3a324185d273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth mean(Y) estimate (if we had access to human annotations on the full text corpus):\n",
      "0.488\n"
     ]
    }
   ],
   "source": [
    "print(\"Ground truth mean(Y) estimate (if we had access to human annotations on the full text corpus):\")\n",
    "print(np.mean(pd.read_csv('data/politeness_dataset.csv').sample(n = N, random_state = random_state)['Prediction_human'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10395a03-bb95-4a1f-9c7c-760459c0a007",
   "metadata": {},
   "source": [
    "#### $\\beta_{hedge}$ estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba13bbd2-e85e-4321-bd65-1be9d7a016fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDI estimate of the target statistic (β: effect of hedging on politeness):\n",
      "-0.0749 1.1171\n"
     ]
    }
   ],
   "source": [
    "lower_bound, upper_bound = confidence_driven_inference(\n",
    "    estimator = log_reg_estimator,\n",
    "    Y = data['human'].values,\n",
    "    Yhat = data['llm'].values,\n",
    "    X = data['X'].values.reshape(-1, 1),\n",
    "    sampling_probs =  data['sampling_probs'].values,\n",
    "    sampling_decisions = data['sampling_decisions'].values)\n",
    "\n",
    "print(\"CDI estimate of the target statistic (β: effect of hedging on politeness):\")\n",
    "print(lower_bound.round(4), upper_bound.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e71d4aa-df44-4e89-8543-9bcd776968e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth β estimate (if we had access to human annotations on the full text corpus):\n",
      "0.2467\n"
     ]
    }
   ],
   "source": [
    "print(\"Ground truth β estimate (if we had access to human annotations on the full text corpus):\")\n",
    "y = pd.read_csv('data/politeness_dataset.csv').sample(n = N, random_state = random_state)['Prediction_human'].values\n",
    "X = pd.read_csv('data/politeness_dataset.csv').sample(n = N, random_state = random_state)[text_based_feature].values\n",
    "\n",
    "print(round(LogisticRegression(solver=\"liblinear\").fit(X.reshape(-1, 1),y).coef_[0, 0],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c25244d6-8afe-4e64-a165-c5a30b170b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human</th>\n",
       "      <th>llm</th>\n",
       "      <th>llm_conf</th>\n",
       "      <th>X</th>\n",
       "      <th>text</th>\n",
       "      <th>sampling_probs</th>\n",
       "      <th>sampling_decisions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1</td>\n",
       "      <td>thank you for that. Do you think it is possibl...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0</td>\n",
       "      <td>Notability is an issue of common sense: you're...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1</td>\n",
       "      <td>Too vague - explain what you expect the conten...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0</td>\n",
       "      <td>I am the third party; &lt;url&gt; was the first to d...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0</td>\n",
       "      <td>Interesting problem.  What have you tried so f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   human  llm  llm_conf  X                                               text  \\\n",
       "0    1.0    1      0.95  1  thank you for that. Do you think it is possibl...   \n",
       "1    0.0    0      0.10  0  Notability is an issue of common sense: you're...   \n",
       "2    0.0    0      0.20  1  Too vague - explain what you expect the conten...   \n",
       "3    0.0    1      0.65  0  I am the third party; <url> was the first to d...   \n",
       "4    1.0    1      0.95  0  Interesting problem.  What have you tried so f...   \n",
       "\n",
       "   sampling_probs  sampling_decisions  \n",
       "0             1.0                 1.0  \n",
       "1             1.0                 1.0  \n",
       "2             1.0                 1.0  \n",
       "3             1.0                 1.0  \n",
       "4             1.0                 1.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58e4de4c-ed83-40f4-8a7d-f8f8f2300f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data collection time: 0.13 minutes.\n"
     ]
    }
   ],
   "source": [
    "elapsed_minutes = (time.time() - start_time_data_collection) / 60\n",
    "print(f\"Total data collection time: {elapsed_minutes:.2f} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
