{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89875d1-3076-44bb-b7a2-200fa497520e",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782ce87f-8c5b-4e0b-a1bb-d592f3312eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, bernoulli\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from ppi_py.utils import bootstrap\n",
    "import re\n",
    "import openai\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import zipfile\n",
    "import io\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "from utils import llms, qualtrics, prolific, mturk, inference\n",
    "from utils.llms import annotate_texts_with_llm, collect_llm_confidence, get_llm_annotations\n",
    "from utils.qualtrics import create_and_activate_surveys\n",
    "from utils.prolific import run_prolific_annotation_pipeline\n",
    "from utils.mturk import run_mturk_annotation_pipeline\n",
    "from utils.inference import train_sampling_rule, sampling_rule_predict, confidence_driven_inference, collect_initial_human_annotations, run_adaptive_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041c91f-ba8c-45a3-8699-b56de27a0398",
   "metadata": {},
   "source": [
    "### Setup credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c47f2f9-a220-47d7-9b39-fea05af70065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_credentials(file_path=\"credentials.txt\"):\n",
    "    credentials = {}\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if '=' in line:\n",
    "                key, value = line.strip().split('=', 1)\n",
    "                credentials[key.strip()] = value.strip()\n",
    "    return credentials\n",
    "\n",
    "# Load credentials, or put your key here in plain text\n",
    "creds = load_credentials()\n",
    "AWS_ACCESS_KEY_ID = creds.get(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = creds.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "OPENAI_API_KEY = creds.get(\"OPENAI_API_KEY\")\n",
    "QUALTRICS_API_KEY = creds.get(\"QUALTRICS_API_KEY\")\n",
    "QUALTRICS_API_URL = creds.get(\"QUALTRICS_API_URL\")\n",
    "PROLIFIC_API_KEY = creds.get(\"PROLIFIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373c599-5e5e-4467-ba2f-ccb6ad81d38c",
   "metadata": {},
   "source": [
    "### Set parameters for Confidence-Driven Inference (CDI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e02c1a9-0d11-47fe-b158-4e2509a7479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if true, we collect LLM annotations and human annotations from scratch, if false, load pre-collected ones\n",
    "COLLECT_LLM = False\n",
    "COLLECT_HUMAN = False\n",
    "\n",
    "# if COLLECT_HUMAN = True, specify whether to use \"Prolific\" or \"MTURK\"\n",
    "HUMAN_SOURCE = \"MTURK\"\n",
    "\n",
    "alpha = 0.1  # desired error level for confidence interval\n",
    "burnin_steps = 5  # we collect the first burnin_steps points to initialize sampling rule\n",
    "\n",
    "n_batches = 2\n",
    "\n",
    "tau = 0.1  # parameter for mixing with uniform sampling for increased stability\n",
    "n_human = 15 # budget on number of human annotations (including burnin_steps)\n",
    "\n",
    "N = 100 # corpus size, or the size of the random subset of the corpus that will be annotated with an LLM\n",
    "random_state = 42\n",
    "\n",
    "#define the estimator function\n",
    "#mask for valid labels and specify how to use weights\n",
    "\n",
    "def mean_estimator(y, weights):\n",
    "    y, weights = y[~np.isnan(y)], weights[~np.isnan(y)]\n",
    "    return np.sum(y * weights) / np.sum(weights)\n",
    "\n",
    "# Load the text corpus; we need two columns: texts and the text-based feature we will use for inference\n",
    "# Text-based feature used for inference in this example is the presence of hedging, stored in \"Feature_3\" column\n",
    "text_based_feature = 'Feature_3'\n",
    "\n",
    "df = pd.read_csv('data/politeness_dataset.csv')[['Text',text_based_feature]]\n",
    "df = df.sample(n = N, random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7403a9-1187-42e7-9e26-5e4feecdcbcd",
   "metadata": {},
   "source": [
    "### Set parameters for LLM annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36cd3394-f0dc-4da4-9826-ba01de5b1d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Is the following text polite? Output either A or B. Output a letter only.\n",
    "A) Polite\n",
    "B) Impolite\n",
    "\n",
    "Text: \"\"\"\n",
    "\n",
    "#category numerically coded as 1\n",
    "positive_class = 'Polite'\n",
    "\n",
    "mapping_categories = {\n",
    "\"A\": \"Polite\",\n",
    "\"B\": \"Impolite\"\n",
    "}\n",
    "\n",
    "model = \"gpt-4o-mini-2024-07-18\"\n",
    "sleep = 0.1\n",
    "temperature = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc1f229-b016-487e-a245-c36e9cccfbbc",
   "metadata": {},
   "source": [
    "### Set parameters for human annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a80cca2-0d6c-41fd-af39-45fa698d27cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task parameters\n",
    "task_title = \"Is the following text polite?\"\n",
    "annotation_instruction = \"Is the following text polite?\"\n",
    "task_description = \"Please annotate the politeness of the provided text. This is a real-time study where we're hoping to get immediate annotations.\"\n",
    "categories = [\n",
    "    \"Polite\",\n",
    "    \"Impolite\"\n",
    "]\n",
    "\n",
    "## Additional Prolific settings\n",
    "BASE_URL = \"https://api.prolific.com/api/v1\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Token {PROLIFIC_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "reward = 80\n",
    "estimated_time = 1\n",
    "maximum_allowed_time = 30 # How long a single annotator can take\n",
    "BATCH_TIMEOUT = 30 # How long we'll wait for the resonses (in minutes) before moving on (replaces not collected annotations with np.nan)\n",
    "\n",
    "## Additional MTURK settings\n",
    "task_reward = '0.8' \n",
    "minimum_approval_rate = 99 # >98% reccommended\n",
    "minimum_tasks_approved = 0 # >5000 reccommended\n",
    "annotation_instructions = {\"question\": task_description,\n",
    "    \"options\": set(categories)}\n",
    "\n",
    "human_annotation_parameters = {\n",
    "    \"categories\": categories,\n",
    "    \"annotation_instruction\": annotation_instruction,\n",
    "    \"annotation_instructions\": annotation_instructions,\n",
    "    \"QUALTRICS_API_URL\": QUALTRICS_API_URL,\n",
    "    \"QUALTRICS_API_KEY\": QUALTRICS_API_KEY,\n",
    "    \"task_title\": task_title,\n",
    "    \"task_description\": task_description,\n",
    "    \"reward\": reward,\n",
    "    \"estimated_time\": estimated_time,\n",
    "    \"maximum_allowed_time\": maximum_allowed_time,\n",
    "    \"HEADERS\": HEADERS,\n",
    "    \"BASE_URL\": BASE_URL,\n",
    "    \"BATCH_TIMEOUT\": BATCH_TIMEOUT,\n",
    "    \"task_reward\": task_reward,\n",
    "    \"minimum_approval_rate\": minimum_approval_rate,\n",
    "    \"minimum_tasks_approved\": minimum_tasks_approved,\n",
    "    \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n",
    "    \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n",
    "    \"positive_class\": positive_class\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef9902-4cd6-45cd-b9e2-24e22cfa1a20",
   "metadata": {},
   "source": [
    "### Step 1: Collect LLM annotations for all the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ea039f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_llm_annotations(df=df,\n",
    "    text_based_feature=text_based_feature,\n",
    "    COLLECT_LLM=COLLECT_LLM,\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    mapping_categories=mapping_categories,\n",
    "    sleep=sleep,\n",
    "    temperature=temperature,\n",
    "    OPENAI_API_KEY=OPENAI_API_KEY,\n",
    "    positive_class=positive_class,\n",
    "    N=N,\n",
    "    random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d13cb2-b36e-452e-bad1-54e68038c54e",
   "metadata": {},
   "source": [
    "### Step 2: Collect warmup human annotations (initial set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08063281",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collect_initial_human_annotations(\n",
    "    data=data,\n",
    "    df=df,\n",
    "    COLLECT_HUMAN=COLLECT_HUMAN,\n",
    "    HUMAN_SOURCE=HUMAN_SOURCE,\n",
    "    burnin_steps=burnin_steps,\n",
    "    N=N,\n",
    "    random_state=random_state,\n",
    "    human_annotation_parameters = human_annotation_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d94f52-61c7-470a-ba92-04112cf58cfb",
   "metadata": {},
   "source": [
    "### Step 3: Strategically collect human annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0ffa9e9-d1af-41d7-ac01-e682110d81b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting batch 1/2...\n",
      "Collecting 2 human annotations.\n",
      "\n",
      "Collecting batch 2/2...\n",
      "Collecting 4 human annotations.\n",
      "11 human datapoints collected in total.\n"
     ]
    }
   ],
   "source": [
    "data = run_adaptive_sampling(\n",
    "    data=data,\n",
    "    df=df,\n",
    "    n=len(df),\n",
    "    burnin_steps=burnin_steps,\n",
    "    n_human=n_human,\n",
    "    n_batches=n_batches,\n",
    "    tau=tau,\n",
    "    COLLECT_HUMAN=COLLECT_HUMAN,\n",
    "    HUMAN_SOURCE=HUMAN_SOURCE,\n",
    "    human_annotation_parameters = human_annotation_parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cac03860-80e5-41ea-9820-4a5a508d6683",
   "metadata": {},
   "source": [
    "### Step 4: Compute the CDI estimate and confidence intervals\n",
    "\n",
    "We showcase estimation of $\\mathrm{mean}(Y)$: prevalence of the politeness, i.e., the fraction of texts in the corpus that are polite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4049879-79e5-44b2-a85e-38c7c1be3569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDI estimate of the target statistic (mean(Y): prevalence of politeness):\n",
      "point estimate: 0.6238\n",
      "confidence intervals: 0.2732 0.8975\n",
      "Ground truth mean(Y) estimate (if we had access to human annotations on the full text corpus):\n",
      "0.58\n"
     ]
    }
   ],
   "source": [
    "estimate, (lower_bound, upper_bound) = confidence_driven_inference(\n",
    "    estimator = mean_estimator,\n",
    "    Y = data['human'].values,\n",
    "    Yhat = data['llm'].values,\n",
    "    sampling_probs =  data['sampling_probs'].values,\n",
    "    sampling_decisions = data['sampling_decisions'].values,\n",
    "    alpha = alpha)\n",
    "\n",
    "print(\"CDI estimate of the target statistic (mean(Y): prevalence of politeness):\")\n",
    "print('point estimate:',estimate.round(4))\n",
    "print('confidence intervals:', lower_bound.round(4), upper_bound.round(4))\n",
    "\n",
    "print(\"Ground truth mean(Y) estimate (if we had access to human annotations on the full text corpus):\")\n",
    "print(np.mean(pd.read_csv('data/politeness_dataset.csv').sample(n = N, random_state = random_state)['Prediction_human'].values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
