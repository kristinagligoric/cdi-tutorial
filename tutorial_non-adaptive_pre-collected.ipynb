{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30319be7-e7f0-4ca1-aaa3-be73ad78e921",
   "metadata": {},
   "source": [
    "### This is a non-adaptive version of the tutorial, where we collect all the human annotations in a single batch\n",
    "- Texts are randomly selected for human annotations, according to the budget\n",
    "- We then leverage the verbalized confidence scores and limited human annotations for valid statistical inference\n",
    "- We assume human annotations, llm annotations, and llm confidence have already been collected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89875d1-3076-44bb-b7a2-200fa497520e",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782ce87f-8c5b-4e0b-a1bb-d592f3312eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, bernoulli\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from ppi_py.utils import bootstrap\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import zipfile\n",
    "import io\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import inference\n",
    "from utils.inference import train_sampling_rule, sampling_rule_predict, confidence_driven_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c5c3f-f304-440b-a137-96b67757a36f",
   "metadata": {},
   "source": [
    "### Set your file parameters and needed columns\n",
    "\n",
    "1. The file path\n",
    "2. Human labels (textual labels such as \"positive\" or \"negative\", or \"polite\" or \"impolite\")\n",
    "3. LLM labels (textual labels such as \"positive\" or \"negative\", or \"polite\" or \"impolite\")\n",
    "4. LLM confidence (a number between 0 and 1)\n",
    "5. A text-based feature (0 or 1 indicating presence or absence of a textual feature)\n",
    "6. What class will be considered as positive (the class we are focusing on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "078dcc95-7104-4617-9661-15b319b10270",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace with your path and your column names\n",
    "\n",
    "# 1. The file path\n",
    "dataset = 'data/politeness_dataset_human_incomplete.csv'\n",
    "\n",
    "# The name of the column with human labels\n",
    "human_labels = 'Prediction_human'\n",
    "\n",
    "# The name of the column with LLM labels\n",
    "llm_labels = 'Prediction_gpt-4o'\n",
    "\n",
    "# The name of the column with LLM confidence\n",
    "llm_confidence = 'Confidence_gpt-4o'\n",
    "\n",
    "# The name of the column with a text-based feature\n",
    "text_based_feature = 'Feature_3'\n",
    "\n",
    "# What class will be considered as positive (the class we are focusing on)\n",
    "positive_class = 'Polite'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373c599-5e5e-4467-ba2f-ccb6ad81d38c",
   "metadata": {},
   "source": [
    "### Set parameters for Confidence-Driven Inference (CDI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e02c1a9-0d11-47fe-b158-4e2509a7479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # desired error level for confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef9902-4cd6-45cd-b9e2-24e22cfa1a20",
   "metadata": {},
   "source": [
    "### Step 1: Load the texts and LLM annotations\n",
    "\n",
    "#### We will showcase the estimation of two target statistics:\n",
    "1. $mean(Y)$, i.e., the fraction of texts in the corpus that are labeled as the positive class\n",
    "2. The impact of text-based feature ($X$), on the positive class annotation ($Y$), estimated with a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cfc30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(pd.read_csv(dataset))\n",
    "data = pd.DataFrame()\n",
    "\n",
    "#load the text-based feature\n",
    "data['X'] = pd.read_csv(dataset)[text_based_feature].values\n",
    "\n",
    "#load the extisting LLM annotations we already collected\n",
    "data['llm'] = pd.read_csv(dataset)[llm_labels].apply(lambda x: 1 if x.lower()==positive_class.lower() else 0).values\n",
    "data['llm_conf'] = pd.read_csv(dataset)[llm_confidence].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d13cb2-b36e-452e-bad1-54e68038c54e",
   "metadata": {},
   "source": [
    "### Step 2: Load human annotations (in a single batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08063281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 human datapoints collected in total.\n"
     ]
    }
   ],
   "source": [
    "#load the extisting human annotations we already collected\n",
    "data['human'] = pd.read_csv(dataset)[[human_labels]].values\n",
    "\n",
    "n_human = sum(~data['human'].isna())\n",
    "print(f\"{len(data.dropna(subset = ['human']))} human datapoints collected in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b242ad9-9bc5-4c97-a83c-b80434e6cef1",
   "metadata": {},
   "source": [
    "### Step 3: Based on LLM's confidence and limited human annotations, learn the weights for all the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ec69d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = data['llm_conf'].to_numpy().reshape((N,1))\n",
    "confidence_human = confidence[~data['human'].isna()]\n",
    "\n",
    "H = data['human'].to_numpy()\n",
    "H_human = H[~data['human'].isna()]\n",
    "\n",
    "Hhat = data['llm'].to_numpy()\n",
    "Hhat_human = Hhat[~data['human'].isna()]\n",
    "\n",
    "data['sampling_decisions'] = (~data['human'].isna()).astype(int)\n",
    "\n",
    "sampling_rule = train_sampling_rule(confidence_human, (H_human - Hhat_human)**2)\n",
    "sampling_probs_unnormed = np.clip(np.sqrt(sampling_rule_predict(sampling_rule, confidence)), 1e-4, 1)\n",
    "data['sampling_probs'] = sampling_probs_unnormed / sum(sampling_probs_unnormed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cac03860-80e5-41ea-9820-4a5a508d6683",
   "metadata": {},
   "source": [
    "### Step 4: Compute the estimate and confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05da928d-6afa-4ab8-a875-306c75ed068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the estimator function\n",
    "#mask for valid labels and specify how to use weights\n",
    "\n",
    "def mean_estimator(y, weights):\n",
    "    y, weights = y[~np.isnan(y)], weights[~np.isnan(y)]\n",
    "    return np.sum(y * weights) / np.sum(weights)\n",
    "\n",
    "def log_reg_estimator(X, y, weights):\n",
    "    X, y, weights = X[~np.isnan(y)], y[~np.isnan(y)], weights[~np.isnan(y)]\n",
    "    return LogisticRegression(solver=\"liblinear\").fit(X, y, sample_weight=weights).coef_[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d935732a-18af-42ff-848c-3929595fe0ef",
   "metadata": {},
   "source": [
    "#### $mean(Y)$ estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4049879-79e5-44b2-a85e-38c7c1be3569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDI estimate of the target statistic (mean(Y)):\n",
      "0.565 0.5974\n"
     ]
    }
   ],
   "source": [
    "lower_bound, upper_bound = confidence_driven_inference(\n",
    "    estimator = mean_estimator,\n",
    "    Y = data['human'].values,\n",
    "    Yhat = data['llm'].values,\n",
    "    sampling_probs =  data['sampling_probs'].values,\n",
    "    sampling_decisions = data['sampling_decisions'].values)\n",
    "\n",
    "print(\"CDI estimate of the target statistic (mean(Y)):\")\n",
    "print(lower_bound.round(4), upper_bound.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10395a03-bb95-4a1f-9c7c-760459c0a007",
   "metadata": {},
   "source": [
    "#### $\\beta$ estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba13bbd2-e85e-4321-bd65-1be9d7a016fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDI estimate of the target statistic (β: effect of X on Y):\n",
      "0.3305 0.6436\n"
     ]
    }
   ],
   "source": [
    "lower_bound, upper_bound = confidence_driven_inference(\n",
    "    estimator = log_reg_estimator,\n",
    "    Y = data['human'].values,\n",
    "    Yhat = data['llm'].values,\n",
    "    X = data['X'].values.reshape(-1, 1),\n",
    "    sampling_probs =  data['sampling_probs'].values,\n",
    "    sampling_decisions = data['sampling_decisions'].values)\n",
    "\n",
    "print(\"CDI estimate of the target statistic (β: effect of X on Y):\")\n",
    "print(lower_bound.round(4), upper_bound.round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
